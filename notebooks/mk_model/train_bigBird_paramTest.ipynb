{"cells":[{"cell_type":"code","execution_count":2,"id":"31ecb381-14e3-4669-adc3-158adc833fce","metadata":{"execution":{"iopub.execute_input":"2024-01-12T22:05:52.639400Z","iopub.status.busy":"2024-01-12T22:05:52.638906Z","iopub.status.idle":"2024-01-12T22:05:52.648222Z","shell.execute_reply":"2024-01-12T22:05:52.647022Z","shell.execute_reply.started":"2024-01-12T22:05:52.639361Z"},"id":"31ecb381-14e3-4669-adc3-158adc833fce"},"outputs":[],"source":["import os\n","import torch\n","from torch.optim import Adam\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import (BigBirdForSequenceClassification, BigBirdTokenizer\n","                          , Trainer, TrainingArguments, DataCollatorWithPadding\n","                         , logging)\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import time\n","import sentencepiece\n","from datasets import load_metric\n","from tqdm import tqdm  # Import tqdm for the progress bar\n","from itertools import product\n","import glob\n","import numpy as np\n"]},{"cell_type":"code","execution_count":7,"id":"caa07b9b-b8c5-45fd-87c5-792d2b4a4e52","metadata":{"execution":{"iopub.execute_input":"2024-01-12T21:22:24.025348Z","iopub.status.busy":"2024-01-12T21:22:24.024828Z","iopub.status.idle":"2024-01-12T21:22:24.035854Z","shell.execute_reply":"2024-01-12T21:22:24.034721Z","shell.execute_reply.started":"2024-01-12T21:22:24.025320Z"},"id":"caa07b9b-b8c5-45fd-87c5-792d2b4a4e52","outputId":"7cb96f94-c112-46c3-9d5c-38221dfebae3"},"outputs":[{"name":"stdout","output_type":"stream","text":["../../data/saved_models/testModelA1_132000_cd0_hd0.2_wd0.1\n"]}],"source":["# set paths\n","data_dir = '../../data/processed'\n","posts_equal_csv = os.path.join(data_dir,'aita_equal.csv')\n","\n","# dump proportion of data (for testing, or if training would take too long with all data)\n","dump_data_prop = 0\n","\n","model = 6\n","# param_search_dict = {\n","#     \"classifier_dropout\":[0,.5]\n","#     , \"hidden_dropout\":[0,.5]\n","#     , \"weight_decay\":[0,.5]\n","# }\n","\n","# count = 0\n","# for i in product(param_search_dict[\"classifier_dropout\"]\n","#                  ,param_search_dict[\"hidden_dropout\"]\n","#                  ,param_search_dict[\"weight_decay\"]):\n","#     if model == count:\n","#         classifier_dropout = i[0]\n","#         hidden_dropout = i[1]\n","#         weight_decay = i[2]\n","#     count += 1\n","\n","# model_save_path=(\n","#     f'./saved_models/testModelB{model}_66000_cd{classifier_dropout}_hd{hidden_dropout}_wd{weight_decay}'\n","# )\n","\n","model_dict = {\n","    \"B1\":[0,0,.5]\n","    , \"A3\":[.2,.2,.1]\n","    , \"C1\":[.2,.2,.5]\n","    , \"3\":[0,.2,.01]\n","    , \"6\":[.2,.2,0]\n","    , \"7\":[.2,.2,.01]\n","    , \"A1\":[0,.2,.1]\n","    , \"B5\":[.5,0,.5]\n","    , \"2\":[0,.2,0]\n","}\n","\n","count = 0\n","for model_name, param_list in model_dict.items():\n","\n","    if model == count:\n","        classifier_dropout = param_list[0]\n","        hidden_dropout = param_list[1]\n","        weight_decay = param_list[2]\n","        model_save_path=(\n","            f'../../data/saved_models/testModel{model_name}_132000_cd{classifier_dropout}_hd{hidden_dropout}_wd{weight_decay}'\n","    )\n","    count += 1\n","\n","print(model_save_path)"]},{"cell_type":"code","execution_count":8,"id":"634482f3-4265-47da-821f-19fb177b1b14","metadata":{"execution":{"iopub.execute_input":"2024-01-12T21:22:25.895741Z","iopub.status.busy":"2024-01-12T21:22:25.895377Z","iopub.status.idle":"2024-01-12T21:22:29.435039Z","shell.execute_reply":"2024-01-12T21:22:29.433633Z","shell.execute_reply.started":"2024-01-12T21:22:25.895712Z"},"id":"634482f3-4265-47da-821f-19fb177b1b14"},"outputs":[],"source":["# Load  from CSV\n","aita_df = pd.read_csv(posts_equal_csv)"]},{"cell_type":"code","execution_count":9,"id":"7688ba8f-4234-4c3c-9203-2de36bd5e1f5","metadata":{"execution":{"iopub.execute_input":"2024-01-12T21:22:29.437316Z","iopub.status.busy":"2024-01-12T21:22:29.437027Z","iopub.status.idle":"2024-01-12T21:22:29.465837Z","shell.execute_reply":"2024-01-12T21:22:29.464408Z","shell.execute_reply.started":"2024-01-12T21:22:29.437289Z"},"id":"7688ba8f-4234-4c3c-9203-2de36bd5e1f5","outputId":"9fd0ef8d-024b-47ea-c96b-50c2854b335d"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU is available.\n"]}],"source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")  # GPU\n","    print(\"GPU is available.\")\n","else:\n","    device = torch.device(\"cpu\")  # CPU\n","    print(\"GPU is not available. Using CPU.\")\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":10,"id":"cc3a5fd4-3969-4cf5-b1e2-0f9fa822fd7c","metadata":{"execution":{"iopub.execute_input":"2024-01-12T21:22:29.468965Z","iopub.status.busy":"2024-01-12T21:22:29.467760Z","iopub.status.idle":"2024-01-12T21:22:29.966551Z","shell.execute_reply":"2024-01-12T21:22:29.965335Z","shell.execute_reply.started":"2024-01-12T21:22:29.468921Z"},"id":"cc3a5fd4-3969-4cf5-b1e2-0f9fa822fd7c"},"outputs":[],"source":["# Preprocessing\n","\n","# recode flair\n","aita_df['label'] = [1 if x == 'YTA' else 0 for x in aita_df['link_flair_text']]\n","\n","x = aita_df['title'] + ' ' + aita_df['selftext']\n","y = aita_df['label']"]},{"cell_type":"code","execution_count":11,"id":"d9f8476f-a961-495b-8a4a-8506464f4dee","metadata":{"execution":{"iopub.execute_input":"2024-01-12T21:22:29.969569Z","iopub.status.busy":"2024-01-12T21:22:29.969252Z","iopub.status.idle":"2024-01-12T21:22:30.065599Z","shell.execute_reply":"2024-01-12T21:22:30.064339Z","shell.execute_reply.started":"2024-01-12T21:22:29.969541Z"},"id":"d9f8476f-a961-495b-8a4a-8506464f4dee","outputId":"94bdb3a3-53ab-47fc-f644-13f5a8b784d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Size: 132000 Val Size: 10113 Test Size: 10113\n"]}],"source":["# split out some if not using whole dataset\n","if dump_data_prop > 0:\n","  X_use, X_dump, y_use, y_dump = train_test_split(\n","      x,y, stratify=y, test_size=dump_data_prop, random_state=42)\n","else:\n","  X_use = x\n","  y_use = y\n","\n","# split out train from val+test\n","X_train, X_hold, y_train, y_hold = train_test_split(\n","    X_use,y_use, stratify=y_use, test_size=0.132865, random_state=42)\n","\n","# split out val and test\n","X_val, X_test, y_val, y_test = train_test_split(\n","    X_hold,y_hold, stratify=y_hold, test_size=0.5, random_state=42)\n","\n","# only train on part of the data for param testing\n","# X_train, X_dump2, y_train, y_dump2 = train_test_split(\n","#     X_train, y_train, stratify=y_train, test_size=0.5, random_state=42)\n","\n","print(f\"Train Size: {len(X_train)} Val Size: {len(X_val)} Test Size: {len(X_test)}\")"]},{"cell_type":"code","execution_count":12,"id":"31ffe0ba-783e-40d3-80a7-a3476594c38f","metadata":{"colab":{"referenced_widgets":["312a50c91eb644e69930ba6a3213debf","0744caa5a4cb4a0c86445262000a5430","e9d86a76b4ac432ea172b9cb52620e81","d6db78b668e54168a3e87135367e2679"]},"execution":{"iopub.execute_input":"2024-01-12T21:22:30.067349Z","iopub.status.busy":"2024-01-12T21:22:30.067026Z","iopub.status.idle":"2024-01-12T21:29:25.927372Z","shell.execute_reply":"2024-01-12T21:29:25.925942Z","shell.execute_reply.started":"2024-01-12T21:22:30.067309Z"},"id":"31ffe0ba-783e-40d3-80a7-a3476594c38f","outputId":"19fe09dd-e0a8-4a17-b89d-bd83cfe45c85"},"outputs":[{"data":{"text/plain":["7026     AITA for calling my grandma an alcoholic? Grow...\n","74236    WIBTA if I confronted my best friend about her...\n","dtype: object"]},"metadata":{},"output_type":"display_data"}],"source":["# load tokenizer and split data into train/validate/test\n","tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')\n","x_train_tokens = tokenizer(list(X_train), padding=True, truncation=True, return_tensors=\"pt\")\n","x_val_tokens = tokenizer(list(X_val), padding=True, truncation=True, return_tensors=\"pt\")\n","x_test_tokens = tokenizer(list(X_test), padding=True, truncation=True, return_tensors=\"pt\")\n","\n","y_train_tensor = torch.tensor(list(y_train), dtype=torch.float)\n","y_val_tensor = torch.tensor(list(y_val), dtype=torch.float)\n","y_test_tensor = torch.tensor(list(y_test), dtype=torch.float)\n","\n","\n","display(X_train[0:2])"]},{"cell_type":"code","execution_count":13,"id":"f64678a2-92f2-42ea-a9ef-c42d9aca96b9","metadata":{"execution":{"iopub.execute_input":"2024-01-12T21:29:25.929592Z","iopub.status.busy":"2024-01-12T21:29:25.929153Z","iopub.status.idle":"2024-01-12T21:29:25.938330Z","shell.execute_reply":"2024-01-12T21:29:25.936765Z","shell.execute_reply.started":"2024-01-12T21:29:25.929549Z"},"id":"f64678a2-92f2-42ea-a9ef-c42d9aca96b9","outputId":"04be0104-98ad-49fb-80af-e143c0c901e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Maximum Sequence Length: 1653\n","Minimum Sequence Length: 1653\n"]}],"source":["# Calculate the maximum and minimum sequence lengths\n","max_sequence_length = x_train_tokens['input_ids'].shape[1]\n","min_sequence_length = x_train_tokens['input_ids'].shape[1]\n","\n","print(f\"Maximum Sequence Length: {max_sequence_length}\")\n","print(f\"Minimum Sequence Length: {min_sequence_length}\")"]},{"cell_type":"code","execution_count":14,"id":"0c080dc2-fa30-4a4b-9dae-5387c5cef450","metadata":{"execution":{"iopub.execute_input":"2024-01-12T21:29:25.939989Z","iopub.status.busy":"2024-01-12T21:29:25.939710Z","iopub.status.idle":"2024-01-12T21:29:25.946301Z","shell.execute_reply":"2024-01-12T21:29:25.945001Z","shell.execute_reply.started":"2024-01-12T21:29:25.939963Z"},"id":"0c080dc2-fa30-4a4b-9dae-5387c5cef450"},"outputs":[],"source":["# prep data\n","train_dataset = TensorDataset(x_train_tokens['input_ids'], x_train_tokens['attention_mask'], y_train_tensor)\n","val_dataset = TensorDataset(x_val_tokens['input_ids'], x_val_tokens['attention_mask'], y_val_tensor)\n","test_dataset = TensorDataset(x_test_tokens['input_ids'], x_test_tokens['attention_mask'], y_test_tensor)"]},{"cell_type":"code","execution_count":15,"id":"7266555a-2322-4bef-9c5d-2c74a718e020","metadata":{"colab":{"referenced_widgets":["9a139ad6ebf249a3b8f3a716c3f36b10"]},"execution":{"iopub.execute_input":"2024-01-12T21:29:25.947777Z","iopub.status.busy":"2024-01-12T21:29:25.947460Z","iopub.status.idle":"2024-01-12T21:29:40.346868Z","shell.execute_reply":"2024-01-12T21:29:40.345791Z","shell.execute_reply.started":"2024-01-12T21:29:25.947751Z"},"id":"7266555a-2322-4bef-9c5d-2c74a718e020","outputId":"93a14be5-60a9-424d-9a6b-2d13e4b0b546"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# Create the model\n","model = BigBirdForSequenceClassification.from_pretrained(\n","    'google/bigbird-roberta-base'\n","    , num_labels=1\n","    , classifier_dropout = classifier_dropout\n","    , hidden_dropout_prob = hidden_dropout\n",")\n","model = model.to(device)\n","optimizer = Adam(model.parameters(), lr=1e-5)\n","loss_fn = nn.BCEWithLogitsLoss()"]},{"cell_type":"code","execution_count":16,"id":"9cca96e8-f2d3-4dc8-a604-2b6bd7d7bc0f","metadata":{"colab":{"referenced_widgets":["d43fc7d1fb2d47d6a11beec15a54e934"]},"execution":{"iopub.execute_input":"2024-01-12T21:29:40.348366Z","iopub.status.busy":"2024-01-12T21:29:40.348109Z","iopub.status.idle":"2024-01-12T21:29:40.865108Z","shell.execute_reply":"2024-01-12T21:29:40.864360Z","shell.execute_reply.started":"2024-01-12T21:29:40.348342Z"},"id":"9cca96e8-f2d3-4dc8-a604-2b6bd7d7bc0f","outputId":"9da171cb-cb5e-4566-f3d0-b010d9a461f4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_377830/2522787558.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(\"accuracy\")\n","/home/walkerped/anaconda3/envs/aita/lib/python3.12/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n","  warnings.warn(\n","Downloading builder script: 4.21kB [00:00, 14.9MB/s]                   "]},{"name":"stdout","output_type":"stream","text":["../../data/saved_models/testModelA1_132000_cd0_hd0.2_wd0.1\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# create accuracy metric\n","metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(p):\n","    return metric.compute(predictions=p.predictions > 0.5, references=p.label_ids)\n","\n","# Define TrainingArguments\n","training_args = TrainingArguments(\n","    output_dir=model_save_path,\n","    num_train_epochs=5,\n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=2,\n","    warmup_steps=1000,\n","    weight_decay=weight_decay,\n","    evaluation_strategy=\"steps\",\n","    save_strategy=\"steps\",\n","    eval_steps=33000,\n","    save_steps=5500,\n","    logging_steps=33000,\n","    load_best_model_at_end=False,\n","    save_total_limit=1,\n","    learning_rate=1e-5,\n","    fp16=True,\n","    report_to='none',\n","    seed=42\n",")\n","\n","# trainer.log_metrics(\"train\", compute_metrics)\n","\n","print(model_save_path)"]},{"cell_type":"code","execution_count":17,"id":"0d427b3f-9023-47c2-a5d1-454e8e257dd3","metadata":{"execution":{"iopub.execute_input":"2024-01-12T21:29:40.868646Z","iopub.status.busy":"2024-01-12T21:29:40.868351Z","iopub.status.idle":"2024-01-12T21:29:40.881539Z","shell.execute_reply":"2024-01-12T21:29:40.880573Z","shell.execute_reply.started":"2024-01-12T21:29:40.868619Z"},"id":"0d427b3f-9023-47c2-a5d1-454e8e257dd3","outputId":"739db714-2783-4734-a006-8c5e31f4947e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/walkerped/anaconda3/envs/aita/lib/python3.12/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]}],"source":["def data_collector(features):\n","    batch = {}\n","    batch['input_ids'] = torch.stack([f[0] for f in features])\n","    batch['attention_mask'] = torch.stack([f[1] for f in features])\n","    batch['labels'] = torch.stack([f[2] for f in features])\n","\n","    return batch\n","\n","# Initialize Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    data_collator=data_collector,\n","    compute_metrics=compute_metrics\n",")\n"]},{"cell_type":"code","execution_count":18,"id":"1d7b27b8-d742-4b6c-ab9d-367b4a1b7db1","metadata":{"colab":{"referenced_widgets":["f63e2b4fe5c144408ae8c91a3fa61a79"]},"execution":{"iopub.execute_input":"2024-01-12T21:29:40.885622Z","iopub.status.busy":"2024-01-12T21:29:40.885379Z","iopub.status.idle":"2024-01-12T21:29:46.903026Z","shell.execute_reply":"2024-01-12T21:29:46.901883Z","shell.execute_reply.started":"2024-01-12T21:29:40.885597Z"},"id":"1d7b27b8-d742-4b6c-ab9d-367b4a1b7db1","outputId":"bb284fe6-a21b-4628-c9d0-aa50ee3476c9"},"outputs":[{"name":"stderr","output_type":"stream","text":["  3%|â–Ž         | 10667/330000 [1:47:26<54:34:41,  1.63it/s]"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m logging\u001b[38;5;241m.\u001b[39mset_verbosity_error()\n\u001b[0;32m----> 8\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain(resume_from_checkpoint \u001b[38;5;241m=\u001b[39m resume)\n","File \u001b[0;32m~/anaconda3/envs/aita/lib/python3.12/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1781\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1782\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1783\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1784\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1785\u001b[0m     )\n","File \u001b[0;32m~/anaconda3/envs/aita/lib/python3.12/site-packages/transformers/trainer.py:2181\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2178\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;66;03m# Optimizer step\u001b[39;00m\n\u001b[0;32m-> 2181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m   2182\u001b[0m optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n\u001b[1;32m   2183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run:\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/aita/lib/python3.12/site-packages/accelerate/optimizer.py:136\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_patched_step_method\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, closure)\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called:\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;66;03m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/aita/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:452\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    450\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 452\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    454\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n","File \u001b[0;32m~/anaconda3/envs/aita/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:350\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 350\u001b[0m     retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n","File \u001b[0;32m~/anaconda3/envs/aita/lib/python3.12/site-packages/accelerate/optimizer.py:191\u001b[0m, in \u001b[0;36mpatch_optimizer_step.<locals>.patched_step\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpatched_step\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    190\u001b[0m     accelerated_optimizer\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/anaconda3/envs/aita/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/anaconda3/envs/aita/lib/python3.12/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/aita/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n","File \u001b[0;32m~/anaconda3/envs/aita/lib/python3.12/site-packages/torch/optim/adamw.py:187\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    174\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    176\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    177\u001b[0m         group,\n\u001b[1;32m    178\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m         state_steps,\n\u001b[1;32m    185\u001b[0m     )\n\u001b[0;32m--> 187\u001b[0m     adamw(\n\u001b[1;32m    188\u001b[0m         params_with_grad,\n\u001b[1;32m    189\u001b[0m         grads,\n\u001b[1;32m    190\u001b[0m         exp_avgs,\n\u001b[1;32m    191\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    192\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    193\u001b[0m         state_steps,\n\u001b[1;32m    194\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    195\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    196\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    197\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    198\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    199\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    200\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    201\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    202\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    203\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    204\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    205\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    206\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    207\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m~/anaconda3/envs/aita/lib/python3.12/site-packages/torch/optim/adamw.py:318\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# Respect when the user inputs False/True for foreach or fused. We only want to change\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# the default when neither have been user-specified. Note that we default to foreach\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# and pass False to use_fused. This is not a mistake--we want to give the fused impl\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# bake-in time before making it the default, even if it is typically faster.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     _, foreach \u001b[38;5;241m=\u001b[39m _default_to_fused_or_foreach(params, differentiable, use_fused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# Do not flip on foreach for the unsupported case where lr is a Tensor and capturable=False.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m foreach \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lr, Tensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m capturable:\n","File \u001b[0;32m~/anaconda3/envs/aita/lib/python3.12/site-packages/torch/optim/optimizer.py:122\u001b[0m, in \u001b[0;36m_default_to_fused_or_foreach\u001b[0;34m(params, differentiable, use_fused)\u001b[0m\n\u001b[1;32m    116\u001b[0m foreach_supported_devices \u001b[38;5;241m=\u001b[39m _get_foreach_kernels_supported_devices()\n\u001b[1;32m    117\u001b[0m fused \u001b[38;5;241m=\u001b[39m use_fused \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    118\u001b[0m     p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mtype\u001b[39m(p) \u001b[38;5;129;01min\u001b[39;00m _foreach_supported_types \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    119\u001b[0m                   p\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m fused_supported_devices \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    120\u001b[0m                   torch\u001b[38;5;241m.\u001b[39mis_floating_point(p)) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params\n\u001b[1;32m    121\u001b[0m )\n\u001b[0;32m--> 122\u001b[0m foreach \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m fused \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    123\u001b[0m     p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mtype\u001b[39m(p) \u001b[38;5;129;01min\u001b[39;00m _foreach_supported_types \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    124\u001b[0m                   p\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m foreach_supported_devices) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fused, foreach\n","File \u001b[0;32m~/anaconda3/envs/aita/lib/python3.12/site-packages/torch/optim/optimizer.py:123\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    116\u001b[0m foreach_supported_devices \u001b[38;5;241m=\u001b[39m _get_foreach_kernels_supported_devices()\n\u001b[1;32m    117\u001b[0m fused \u001b[38;5;241m=\u001b[39m use_fused \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    118\u001b[0m     p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mtype\u001b[39m(p) \u001b[38;5;129;01min\u001b[39;00m _foreach_supported_types \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    119\u001b[0m                   p\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m fused_supported_devices \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    120\u001b[0m                   torch\u001b[38;5;241m.\u001b[39mis_floating_point(p)) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params\n\u001b[1;32m    121\u001b[0m )\n\u001b[1;32m    122\u001b[0m foreach \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m fused \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m--> 123\u001b[0m     p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mtype\u001b[39m(p) \u001b[38;5;129;01min\u001b[39;00m _foreach_supported_types \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    124\u001b[0m                   p\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m foreach_supported_devices) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fused, foreach\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["if glob.glob(f\"{model_save_path}/checkpoint*/pytorch_model.bin\"):\n","    resume = True\n","else:\n","    resume = False\n","\n","# train the model\n","logging.set_verbosity_error()\n","trainer.train(resume_from_checkpoint = resume)"]},{"cell_type":"code","execution_count":null,"id":"08949982-20a3-49a3-976c-470587722bb6","metadata":{"id":"08949982-20a3-49a3-976c-470587722bb6"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":5}
